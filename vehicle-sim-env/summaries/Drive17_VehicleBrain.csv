Steps,Policy/Entropy,Policy/Learning Rate,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Environment/Cumulative Reward,Environment/Episode Length
1000,1.4189383,0.00029940062,0.14800966,144.7300090789795,144.73000074923038,337.0
2000,1.4189383,0.0002982006,0.15118432,144.3133398691813,144.31333427627882,340.0
3000,1.4189383,0.00029700057,0.20969316,157.3300083478292,157.33000048001608,361.3333333333333
4000,1.4189383,0.00029580062,0.15869294,163.57000875473022,163.5700006186962,377.5
5000,1.4189383,0.0002946006,0.20757182,183.23001116514206,183.23000046610832,418.5
6000,1.4189383,0.0002934006,0.20926914,165.54334163665771,165.5433340271314,383.0
7000,1.4189383,0.00029220062,0.18663502,151.92334143320718,151.92333420117697,355.6666666666667
8000,1.4189383,0.00029100062,0.15087286,165.21001052856445,165.21000091731548,386.0
9000,1.4189383,0.0002898006,0.21658725,194.22001361846924,194.2200015038252,459.5
10000,1.4189383,0.00028860063,0.19843674,145.71334060033163,145.71333372592926,334.3333333333333
11000,1.4189383,0.00028740062,0.22802362,190.1400113105774,190.14000158011913,452.0
