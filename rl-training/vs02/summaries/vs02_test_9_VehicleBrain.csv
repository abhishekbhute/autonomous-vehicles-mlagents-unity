Steps,Policy/Entropy,Policy/Learning Rate,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Environment/Cumulative Reward,Environment/Episode Length,Losses/Value Loss,Losses/Policy Loss
10000,1.0974345,9.990001e-05,0.6051438,213.73552248307638,213.73552156399404,676.7857142857143,46.511326,0.8496026
20000,1.0935935,9.970001e-05,1.9487877,254.8092370203563,254.80923981485623,669.4285714285714,46.724617,0.8227226
30000,1.086704,9.950001e-05,3.5388455,223.5502312845654,223.55023141081134,552.7777777777778,47.75226,0.83199036
40000,1.0853286,9.930001e-05,4.8749595,189.28384217992425,189.283840023214,564.4375,47.431904,0.8739217
50000,1.087504,9.9100005e-05,3.3423033,117.41589219319194,117.41589345704568,511.57894736842104,46.87967,0.9082818
60000,1.0751297,9.890001e-05,5.3541236,159.89851499858656,159.89851231127977,520.1052631578947,47.483295,0.86130303
70000,1.0765744,9.87e-05,9.080736,262.9068273987089,262.90682540408204,662.7142857142857,45.691708,0.78813064
80000,1.0806338,9.850001e-05,8.003163,167.79588690825872,167.7958866020753,466.95238095238096,47.698753,0.8696059
